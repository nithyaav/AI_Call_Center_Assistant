# Call Center AI Assistant ğŸ“

A sophisticated multi-agent system for automated call center analytics using **LangGraph**, **LangChain**, **OpenAI GPT-4**, and **Streamlit**. Features persistent data storage, agent performance tracking, and quality analytics.

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![LangGraph](https://img.shields.io/badge/LangGraph-0.2.45-green.svg)](https://github.com/langchain-ai/langgraph)
[![LangChain](https://img.shields.io/badge/LangChain-0.3.9-orange.svg)](https://github.com/langchain-ai/langchain)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.40.1-red.svg)](https://streamlit.io/)

</div>

## ğŸ¯ Business Use Case

In modern call centers, crucial insights from conversations are often trapped in long transcripts or voice recordings. Manual summaries, quality checks, and agent evaluations are time-consuming and inconsistent.

**This system provides:**
- ğŸ¤– **Automated Insight Extraction**: Rapidly summarize calls without manual effort
- ğŸ“Š **QA Monitoring at Scale**: Score service quality using LLM agents
- ğŸ‘¥ **Agent Performance Tracking**: Monitor and evaluate agents over time
- ğŸ“ˆ **Data-Driven Decisions**: Use analytics for promotions and training
- âœ… **Consistency & Compliance**: Standardize evaluations across interactions
- ğŸ¤ **Voice-to-Insights Pipeline**: Convert audio into structured data for decision-making

## âœ¨ Features

### ï¿½ï¸ **Content Safety Agent** âœ¨ 
- **Automated content moderation** using OpenAI Moderation API
- Checks transcribed text for inappropriate content:
  - Harassment and threatening language
  - Hate speech
  - Violence and graphic content
  - Sexual content
  - Self-harm content
- Flags violations and routes to manual review
- Stores flagged content with FLAGGED_ prefix for easy identification
- Preserves full transcript and metadata for compliance

### ï¿½ğŸ” **Call Intake Agent** âœ¨ 
- **Combined Validation & Metadata Extraction** (50% API cost reduction!)
- Validates input formats and extracts metadata in a single LLM call
- Intelligently parses caller name, agent name, call duration, call ID, date/time
- Structures entire conversation into speaker turns
- **No regex patterns** - pure LLM-based extraction for flexibility
- Graceful handling of missing agent names (saves for manual review)

### ğŸ¤ **Transcription Agent**
- Converts audio to text using OpenAI Whisper
- Supports multiple audio formats (WAV, MP3, M4A, FLAC, OGG)
- Handles files up to 25 MB
- Output fed directly to Content Safety check

### ğŸ“ **Summarization Agent**
- Generates concise summaries and key points using GPT-4
- Identifies customer issues and resolutions
- Extracts action items and follow-ups
- **Intelligent routing**: Determines if quality scoring should proceed

### â­ **Quality Scoring Agent** âœ¨ 
- Evaluates calls on 4 key dimensions:
  - Tone & Empathy (0-10)
  - Professionalism (0-10)
  - Problem Resolution or Call Effectiveness (0-10)
  - Response Appropriateness (0-10)
- **Context-Aware Scoring**: Adapts rubric for problem resolution vs informational calls
- **Smart Fallback Scoring**: 
  - Calculates mean only from successfully extracted scores
  - No arbitrary defaults that bias averages
  - Flags incomplete scoring for manual review
- Provides detailed feedback with strengths and improvement areas
- **Always runs**: Provides insights even when agent name is missing

### ğŸ’¾ **Data Storage Agent** âœ¨ Complete Persistence
- **ALWAYS SAVES DATA** - Zero data loss guarantee!
- Persists ALL calls regardless of:
  - Content safety status (flagged content stored separately)
  - Agent name presence (saves with `needs_manual_review` flag)
  - Quality scoring status (saves even if scoring fails)
- Stores data in multiple formats (JSON, CSV)
- **Selective Analytics Updates**: Only updates agent rankings when attribution is reliable
- Tracks agent performance over time with full audit trail
- Generates comprehensive agent reports with trends
- Supports performance reviews and promotion decisions
- **Manual Review System**: Flags incomplete/unsafe calls for human verification
- **Flagged Content Storage**: Separate files with FLAGGED_ prefix for content safety violations

### ğŸ”„ **Workflow Orchestration (LangGraph)** âœ¨ Enhanced
- Orchestrates workflow between all 6 agents
- **Intelligent Conditional Routing**:
  - Audio vs text input routing
  - Content safety check after transcription
  - Conditional edge: flagged content routes to storage (not blocked)
  - Quality scoring always runs (provides insights even without agent name)
  - **Always routes to storage** - no data loss
- Error handling and state management
- Optimized with `operator.add` for processing steps
- **Cost Optimizations**: Combined validation + extraction (35% reduction)

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9 or higher
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))

### Installation

```bash
# Navigate to project directory
cd call_center_analytics

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On macOS/Linux
# or: venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
# Copy the example file and add your OpenAI API key
cp .env.example .env
# Then edit .env and replace 'your_openai_api_key_here' with your actual key

# Verify setup
python test_setup.py

# Run the application
streamlit run app.py
```

The app will open at `http://localhost:8501`


## ğŸ’» Usage

### Two-Page Interface

#### ğŸ“ **Process Call Page**
1. **Upload File Tab**: Upload audio (.wav, .mp3, .m4a, .flac, .ogg) or text (.txt) files
2. **Paste Text Tab**: Directly paste call transcripts
3. **Last Result Tab** â­ NEW!: View your most recent analysis (persists across navigation)

#### ğŸ‘¥ **Agent Performance Page** â­ ENHANCED!
1. **Overall Statistics**: View total agents, calls, and average scores (when agents are ranked)
2. **Agent Rankings Tab**: See all agents ranked by performance
   - Shows empty state with helpful message when no agents ranked
   - Excludes calls needing manual review for accuracy
3. **Agent Reports Tab**: Generate detailed reports with:
   - Total calls processed
   - Average scores across all categories
   - Performance trends (improving/declining/stable)
   - Performance rating (Outstanding to Unsatisfactory)
   - Recent call history
   - Shows empty state when no agent data available
4. **Manual Review Queue Tab** ğŸ†•: Review flagged content
   - Badge shows count of items needing review
   - Three types of flagged items:
     - ğŸš¨ **Content Safety Violations**: Inappropriate content detected
     - âš ï¸ **Missing Agent Name**: No agent identified
     - âš ï¸ **Incomplete Scoring**: Quality scoring failed
   - Shows full transcript for flagged content
   - Displays flagged categories for content safety violations
   - **Always accessible** even when no agents are ranked

### Processing a Call

**Option 1: Upload Files**
1. Navigate to "ğŸ“ Process Call" page
2. Click "Upload File" tab
3. Upload either:
   - **Text file** (.txt) with call transcript
   - **Audio file** (.wav, .mp3, .m4a, .flac, .ogg)

**Option 2: Paste Text**
1. Navigate to "ğŸ“ Process Call" page
2. Click "Paste Text" tab
3. Paste your call transcript directly
4. Click "Analyze Text"

**Option 3: View Last Result** 
1. Navigate to "ğŸ“ Process Call" page
2. Click "Last Result" tab
3. View your previous analysis (even after switching pages)

### View Results
The system displays:
- ï¿½ï¸ **Content Safety Check**: Notification if content flagged for manual review
- ï¿½ğŸ“‹ **Call Metadata**: ID, names, duration, date/time
- ğŸ’¬ **Conversation Transcript**: Full dialogue with speaker turns
- ğŸ“ **AI-Generated Summary**: Key points, issues, resolution, and action items
- â­ **Quality Scores**: Detailed evaluation with feedback, strengths, and areas for improvement
- âœ… **Storage Confirmation**: Data automatically saved for future analytics
- âš ï¸ **Manual Review Flags**: Alerts for content safety violations, missing agent, or incomplete scoring (data still saved!)

## ğŸ“ Project Structure

```
call_center_analytics/
â”œâ”€â”€ agents/                      # Agent implementations
â”‚   â”œâ”€â”€ __init__.py             # Package initialization
â”‚   â”œâ”€â”€ content_safety_agent.py # Content moderation (NEW!)
â”‚   â”œâ”€â”€ call_intake_agent.py    # LLM-based metadata extraction
â”‚   â”œâ”€â”€ transcription_agent.py  # Audio transcription with Whisper
â”‚   â”œâ”€â”€ summarization_agent.py  # GPT-4 summary generation
â”‚   â”œâ”€â”€ quality_scoring_agent.py # Quality assessment
â”‚   â”œâ”€â”€ data_storage_agent.py   # Data persistence with flagged content handling
â”‚   â””â”€â”€ workflow.py             # LangGraph orchestration with content safety
â”œâ”€â”€ utils/                       # Utilities
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ models.py               # Pydantic data models
â”‚   â”œâ”€â”€ guardrails.py           # Content safety guardrails (NEW!)
â”‚   â””â”€â”€ guardrails_config.py    # Guardrails configuration (NEW!)
â”œâ”€â”€ data_storage_call_center/  # Persistent storage (auto-created)
â”‚   â”œâ”€â”€ calls/                  # Individual call JSON files
â”‚   â”‚   â”œâ”€â”€ CALL_*.json        # Normal calls with agent attribution
â”‚   â”‚   â””â”€â”€ FLAGGED_*.json     # Content safety violations (NEW!)
â”‚   â”œâ”€â”€ reports/                # Agent performance reports
â”‚   â”œâ”€â”€ calls_database.json     # Master call index with needs_manual_review flags
â”‚   â”œâ”€â”€ quality_scores.csv      # All quality scores
â”‚   â”œâ”€â”€ agent_performance.csv   # Agent statistics (excludes manual review calls)
â”‚   â””â”€â”€ transcript_hashes.json  # Duplicate detection (NEW!)
â”œâ”€â”€ sample_data/                # Sample transcripts and audio
â”‚   â”œâ”€â”€ example_transcript_good.txt         # High quality call
â”‚   â”œâ”€â”€ example_transcript_excellent.txt    # Outstanding service
â”‚   â”œâ”€â”€ example_transcript_poor.txt         # Poor quality call
â”‚   â”œâ”€â”€ example_transcript_flagged.txt      # Content safety violation (NEW!)
â”‚   â””â”€â”€ example_transcript_abusive.txt      # Abusive language example (NEW!)
â”œâ”€â”€ app.py                      # Streamlit web interface with Manual Review
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ test_setup.py              # Setup verification script
```

## ğŸ“ Input Format

### Text File Format
```
Call ID: CS-2026-001
Date: 2026-01-05 14:30:00
Duration: 5:23
Caller Name: John Doe
Agent Name: Sarah Johnson

Conversation:
Agent: Thank you for calling. How may I help you today?
Caller: I have an issue with my recent order.
Agent: I'm sorry to hear that...
```

### Audio Files
- Supported: WAV, MP3, M4A, FLAC, OGG
- Max size: 25 MB
- Automatically transcribed

## ğŸ§ª Testing

Test with provided sample data:

```bash
# Run setup verification
python test_setup.py

# Start the app (recommended multi-page version)
streamlit run pages_app.py

# Then upload: sample_data/sample_call_transcript.txt
```

Two sample files provided:
- `sample_call_transcript.txt` - High quality call example
- `sample_call_poor_quality.txt` - Poor quality call example

### Testing Features
1. **Process a normal call** to see metadata extraction, summarization, and quality scoring
2. **Process flagged content** (use `example_transcript_flagged.txt`) to see content safety in action
3. **Switch to Agent Performance** to see the agent added to rankings
4. **Check Manual Review Queue** to see flagged content with categories
5. **Generate Agent Report** to view comprehensive performance analysis
6. **Test without agent name** to see manual review flagging system
7. **Verify Manual Review tab** is accessible even without ranked agents

## ğŸ” Manual Review System

The system intelligently handles incomplete data and content safety violations without losing information:

### When Manual Review is Triggered:
- ğŸš¨ **Content Safety Violation**: Inappropriate content detected by guardrails
- âŒ **No agent name identified** in the call
- âŒ **Quality scoring fails** (< 2 scores extracted)

### What Happens:
1. âœ… **Data is ALWAYS saved** - no information loss
2. ğŸ·ï¸ **Flagged with** `needs_manual_review: true`
3. âš ï¸ **Warning displayed** in UI with clear reasoning
4. ğŸ“Š **Excluded from agent rankings** (ensures accuracy)
5. ğŸ“‹ **Available in Manual Review Queue** for human review
6. ğŸš¨ **Content violations stored** with FLAGGED_ prefix

### Manual Review Scenarios:
| Scenario | Content Safe | Agent Name | Quality Score | Result |
|----------|-------------|-----------|---------------|--------|
| Full Success | âœ… Safe | âœ… Present | âœ… Complete | Saved + Ranked |
| Content Violation | ğŸš¨ Flagged | N/A | â­ï¸ Skipped | Saved for Review (FLAGGED_) |
| No Agent | âœ… Safe | âŒ Missing | âœ… Complete | Saved for Review |
| Scoring Failed | âœ… Safe | âœ… Present | âŒ Incomplete | Saved for Review |
| Both Missing | âœ… Safe | âŒ Missing | âŒ Incomplete | Saved for Review |

### Accessing Manual Review Queue:
1. Navigate to **Agent Performance** page
2. Click on **Manual Review Queue** tab
3. View all flagged items with:
   - ğŸš¨ Content safety violations (with categories)
   - âš ï¸ Missing agent name calls
   - âš ï¸ Incomplete scoring calls
4. Review full transcripts and details
5. Tab badge shows count of items needing review

### Benefits:
- ğŸ”’ **Zero Data Loss**: Every valid conversation is preserved
- ğŸ›¡ï¸ **Content Safety**: Inappropriate content flagged for review
- ğŸ“Š **Accurate Rankings**: Only reliable scores affect agent performance
- ğŸ” **Full Audit Trail**: All calls available for compliance and review
- ğŸ¯ **Clear Attribution**: Manual review resolves ambiguous cases
- ğŸ‘ï¸ **Easy Access**: Manual Review Queue always visible in Agent Performance page

## ğŸ—ï¸ Architecture

The system uses **LangGraph** for multi-agent orchestration with 6 specialized agents:

```
Input (Text/Audio)
        â†“
    [Router] â”€â”€â”€â”€â”€â†’ Audio? â†’ [Transcription Agent]
        â†“                            â†“
    Text? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
[Content Safety Agent] â† Check for inappropriate content (OpenAI Moderation API)
        â†“                 ğŸ†• Routes to storage if flagged, continues if safe
        â”œâ”€â”€â”€â”€â”€â†’ Flagged? â†’ [Data Storage] (FLAGGED_ files)
        â†“
[Call Intake Agent] â† Extract metadata & parse conversation (LLM-based)
        â†“             âœ¨ Combined validation + extraction (50% cost reduction)
[Summarization Agent] â† Generate summary using GPT-4
        â†“
[Quality Scoring Agent] â† Evaluate quality with context-aware rubric
        â†“             âœ¨ Always runs (even without agent name for insights)
        â†“
[Data Storage Agent] â† ALWAYS SAVES DATA (zero loss!)
        â†“             âœ¨ Flags for manual review if incomplete or unsafe
    Results + Analytics + Manual Review Queue
```


## ğŸ’° Cost Estimate

Approximate OpenAI API costs per call (after optimizations):
- **Whisper** (audio transcription): ~$0.006 per minute of audio
- **Moderation API** (content safety): ~$0.002 per call ğŸ†•
- **GPT-4** (combined validation + metadata + summary): ~$0.02-0.04 per call âœ¨ (35% reduction!)
- **GPT-4** (quality scoring, always runs): ~$0.01-0.02 per call
- **Typical 5-minute call**: $0.04 - $0.16 âœ¨ (was $0.05-$0.20)



## ğŸ”’ Security

- âœ… API keys stored in `.env` 
- âœ… Secure file handling with temporary storage for audio
- âœ… Local data storage (no external database required)

## ğŸ› Troubleshooting

### "Import could not be resolved" errors
These are normal before installation. Run `pip install -r requirements.txt` to fix.

### "OPENAI_API_KEY not set"
1. Copy `.env.example` to `.env`
2. Add your OpenAI API key
3. Restart the application

### "Module not found"
Ensure virtual environment is activated:
```bash
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows
```



Start by running: `streamlit run app.py`
